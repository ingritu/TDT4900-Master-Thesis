\hypertarget{classcustom__layers_1_1_attention_layer}{}\doxysection{custom\+\_\+layers.\+Attention\+Layer Class Reference}
\label{classcustom__layers_1_1_attention_layer}\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
Inheritance diagram for custom\+\_\+layers.\+Attention\+Layer\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classcustom__layers_1_1_attention_layer}
\end{center}
\end{figure}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \mbox{\hyperlink{classcustom__layers_1_1_attention_layer_a29a8af355aacf5bf903d2ac3480d8fcf}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, input\+\_\+size, hidden\+\_\+size)
\item 
def \mbox{\hyperlink{classcustom__layers_1_1_attention_layer_aebe9094d7a6b0306323fe84f79448a14}{forward}} (self, x)
\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_a45691265bef09c218612f1b693ac9142}{v\+\_\+att}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_ab5d7414b4d121ac1528468ddeba41f7a}{s\+\_\+proj}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_ac11919745a96624936f6c60a64247d8f}{s\+\_\+att}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_a13877ed62f001c9237f8f361e241bf21}{h\+\_\+proj}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_a2dcc1996663d5dd68a5de76a6487b787}{h\+\_\+att}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_a2bd68f949b6e84ca0f11c5d75e0c495a}{alpha\+\_\+layer}}
\item 
\mbox{\hyperlink{classcustom__layers_1_1_attention_layer_af3086dc3f6a05472346c48ded79428c4}{context\+\_\+proj}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}


Definition at line 112 of file custom\+\_\+layers.\+py.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_a29a8af355aacf5bf903d2ac3480d8fcf}\label{classcustom__layers_1_1_attention_layer_a29a8af355aacf5bf903d2ac3480d8fcf}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def custom\+\_\+layers.\+Attention\+Layer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{input\+\_\+size,  }\item[{}]{hidden\+\_\+size }\end{DoxyParamCaption})}



Definition at line 114 of file custom\+\_\+layers.\+py.


\begin{DoxyCode}{0}
\DoxyCodeLine{114     \textcolor{keyword}{def }\_\_init\_\_(self, input\_size, hidden\_size):}
\DoxyCodeLine{115         super(AttentionLayer, self).\_\_init\_\_()}
\DoxyCodeLine{116         \textcolor{comment}{\# input\_size 512}}
\DoxyCodeLine{117         \textcolor{comment}{\# hidden\_size 512}}
\DoxyCodeLine{118 }
\DoxyCodeLine{119         self.v\_att = nn.Linear(input\_size, hidden\_size)}
\DoxyCodeLine{120 }
\DoxyCodeLine{121         self.s\_proj = nn.Linear(input\_size, input\_size)}
\DoxyCodeLine{122         self.s\_att = nn.Linear(input\_size, hidden\_size)}
\DoxyCodeLine{123 }
\DoxyCodeLine{124         self.h\_proj = nn.Linear(input\_size, input\_size)}
\DoxyCodeLine{125         self.h\_att = nn.Linear(input\_size, hidden\_size)}
\DoxyCodeLine{126 }
\DoxyCodeLine{127         self.alpha\_layer = nn.Linear(hidden\_size, 1)}
\DoxyCodeLine{128         \textcolor{comment}{\# might move this outside}}
\DoxyCodeLine{129         self.context\_proj = nn.Linear(input\_size, input\_size)}
\DoxyCodeLine{130 }

\end{DoxyCode}


\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_aebe9094d7a6b0306323fe84f79448a14}\label{classcustom__layers_1_1_attention_layer_aebe9094d7a6b0306323fe84f79448a14}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!forward@{forward}}
\index{forward@{forward}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def custom\+\_\+layers.\+Attention\+Layer.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{x }\end{DoxyParamCaption})}



Definition at line 131 of file custom\+\_\+layers.\+py.


\begin{DoxyCode}{0}
\DoxyCodeLine{131     \textcolor{keyword}{def }forward(self, x):}
\DoxyCodeLine{132         \textcolor{comment}{\# x : [V, s\_t, h\_t]}}
\DoxyCodeLine{133         \textcolor{comment}{\# V = [v1, v2, ..., vk]}}
\DoxyCodeLine{134         \textcolor{comment}{\# c\_t is context vector: sum of alphas*v}}
\DoxyCodeLine{135         \textcolor{comment}{\# output should be beta*s\_t + (1-\/beta)*c\_t}}
\DoxyCodeLine{136         \textcolor{comment}{\# print('attention layer')}}
\DoxyCodeLine{137         V = x[0]  \textcolor{comment}{\# (batch\_size, 8x8, hidden\_size)}}
\DoxyCodeLine{138         s\_t = x[1]  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{139         h\_t = x[2]  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{140 }
\DoxyCodeLine{141         \textcolor{comment}{\# embed visual features}}
\DoxyCodeLine{142         v\_embed = F.relu(self.v\_att(V))  \textcolor{comment}{\# (batch\_size, 64, hidden\_size)}}
\DoxyCodeLine{143 }
\DoxyCodeLine{144         \textcolor{comment}{\# s\_t embedding}}
\DoxyCodeLine{145         s\_proj = F.relu(self.s\_proj(s\_t))  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{146         s\_att = self.s\_att(s\_proj)  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{147 }
\DoxyCodeLine{148         \textcolor{comment}{\# h\_t embedding}}
\DoxyCodeLine{149         h\_proj = torch.tanh(self.h\_proj(h\_t))  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{150         h\_att = self.h\_att(h\_proj)  \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{151 }
\DoxyCodeLine{152         \textcolor{comment}{\# make s\_proj the same dimension as V}}
\DoxyCodeLine{153         s\_proj = s\_proj.unsqueeze(1)  \textcolor{comment}{\# (batch\_size, 1, hidden\_size)}}
\DoxyCodeLine{154 }
\DoxyCodeLine{155         \textcolor{comment}{\# make s\_att the same dimension as v\_att}}
\DoxyCodeLine{156         s\_att = s\_att.unsqueeze(1)  \textcolor{comment}{\# (batch\_size, 1, hidden\_size)}}
\DoxyCodeLine{157 }
\DoxyCodeLine{158         \textcolor{comment}{\# make h\_att the same dimension as regions\_att}}
\DoxyCodeLine{159         h\_att = h\_att.unsqueeze(1).expand(h\_att.size()[0],}
\DoxyCodeLine{160                                           V.size()[1] + 1,}
\DoxyCodeLine{161                                           h\_att.size()[1])}
\DoxyCodeLine{162         \textcolor{comment}{\# (batch\_size, 64 + 1, hidden\_size)}}
\DoxyCodeLine{163 }
\DoxyCodeLine{164         \textcolor{comment}{\# concatenations}}
\DoxyCodeLine{165         regions = torch.cat((V, s\_proj), dim=1)}
\DoxyCodeLine{166         \textcolor{comment}{\# (batch\_size, 64 +1, hidden\_size)}}
\DoxyCodeLine{167         regions\_att = torch.cat((v\_embed, s\_att), dim=1)}
\DoxyCodeLine{168         \textcolor{comment}{\# (batch\_size, 64 +1, hidden\_size)}}
\DoxyCodeLine{169 }
\DoxyCodeLine{170         \textcolor{comment}{\# add h\_t to regions\_att}}
\DoxyCodeLine{171         alpha\_input = F.tanh(regions\_att + h\_att)}
\DoxyCodeLine{172         \textcolor{comment}{\# (batch\_size, 64 +1, hidden\_size)}}
\DoxyCodeLine{173 }
\DoxyCodeLine{174         \textcolor{comment}{\# compute alphas + beta}}
\DoxyCodeLine{175         alpha = F.softmax(self.alpha\_layer(alpha\_input).squeeze(2), dim=1)}
\DoxyCodeLine{176         \textcolor{comment}{\# (batch\_size, 64 + 1)}}
\DoxyCodeLine{177         alpha = alpha.unsqueeze(2)  \textcolor{comment}{\# (batch\_size, 64 +1, 1)}}
\DoxyCodeLine{178 }
\DoxyCodeLine{179         \textcolor{comment}{\# multiply with regions}}
\DoxyCodeLine{180         context\_vector = (alpha * regions).sum(dim=1)  \textcolor{comment}{\# the actual z\_t}}
\DoxyCodeLine{181         \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{182 }
\DoxyCodeLine{183         z\_t = torch.tanh(self.context\_proj(context\_vector + h\_proj))}
\DoxyCodeLine{184         \textcolor{comment}{\# (batch\_size, hidden\_size)}}
\DoxyCodeLine{185 }
\DoxyCodeLine{186         \textcolor{keywordflow}{return} z\_t}

\end{DoxyCode}


\doxysubsection{Member Data Documentation}
\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_a2bd68f949b6e84ca0f11c5d75e0c495a}\label{classcustom__layers_1_1_attention_layer_a2bd68f949b6e84ca0f11c5d75e0c495a}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!alpha\_layer@{alpha\_layer}}
\index{alpha\_layer@{alpha\_layer}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{alpha\_layer}{alpha\_layer}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+alpha\+\_\+layer}



Definition at line 127 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_af3086dc3f6a05472346c48ded79428c4}\label{classcustom__layers_1_1_attention_layer_af3086dc3f6a05472346c48ded79428c4}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!context\_proj@{context\_proj}}
\index{context\_proj@{context\_proj}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{context\_proj}{context\_proj}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+context\+\_\+proj}



Definition at line 129 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_a2dcc1996663d5dd68a5de76a6487b787}\label{classcustom__layers_1_1_attention_layer_a2dcc1996663d5dd68a5de76a6487b787}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!h\_att@{h\_att}}
\index{h\_att@{h\_att}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{h\_att}{h\_att}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+h\+\_\+att}



Definition at line 125 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_a13877ed62f001c9237f8f361e241bf21}\label{classcustom__layers_1_1_attention_layer_a13877ed62f001c9237f8f361e241bf21}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!h\_proj@{h\_proj}}
\index{h\_proj@{h\_proj}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{h\_proj}{h\_proj}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+h\+\_\+proj}



Definition at line 124 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_ac11919745a96624936f6c60a64247d8f}\label{classcustom__layers_1_1_attention_layer_ac11919745a96624936f6c60a64247d8f}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!s\_att@{s\_att}}
\index{s\_att@{s\_att}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{s\_att}{s\_att}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+s\+\_\+att}



Definition at line 122 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_ab5d7414b4d121ac1528468ddeba41f7a}\label{classcustom__layers_1_1_attention_layer_ab5d7414b4d121ac1528468ddeba41f7a}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!s\_proj@{s\_proj}}
\index{s\_proj@{s\_proj}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{s\_proj}{s\_proj}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+s\+\_\+proj}



Definition at line 121 of file custom\+\_\+layers.\+py.

\mbox{\Hypertarget{classcustom__layers_1_1_attention_layer_a45691265bef09c218612f1b693ac9142}\label{classcustom__layers_1_1_attention_layer_a45691265bef09c218612f1b693ac9142}} 
\index{custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}!v\_att@{v\_att}}
\index{v\_att@{v\_att}!custom\_layers.AttentionLayer@{custom\_layers.AttentionLayer}}
\doxysubsubsection{\texorpdfstring{v\_att}{v\_att}}
{\footnotesize\ttfamily custom\+\_\+layers.\+Attention\+Layer.\+v\+\_\+att}



Definition at line 119 of file custom\+\_\+layers.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/models/\mbox{\hyperlink{custom__layers_8py}{custom\+\_\+layers.\+py}}\end{DoxyCompactItemize}
